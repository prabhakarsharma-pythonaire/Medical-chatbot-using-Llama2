print("Ok")


from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEmbeddings  # Updated import
from langchain_pinecone import Pinecone  # Updated import
from pinecone import Pinecone as PineconeClient
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.llms import CTransformers
import os
from dotenv import load_dotenv
load_dotenv()



# Set your Pinecone API key and environment
PINECONE_API_KEY = "00c69ec0-2ec0-43a4-9b98-9f5a093d6ea4"
PINECONE_ENV = "us-east-1-aws"
PINECONE_INDEX_NAME = "medical-chatbot"



# Initialize Pinecone client
pinecone = PineconeClient(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)

# Force Index Refresh (Important!)
pinecone.describe_index(PINECONE_INDEX_NAME)

print(pinecone.list_indexes())


# Check if index exists, create if not
# Get list of index names
index_names = [index['name'] for index in pinecone.list_indexes()]

# Check if index exists, create if not (Modified)
if PINECONE_INDEX_NAME not in index_names:
    pinecone.create_index(
        PINECONE_INDEX_NAME,
        dimension=384,
        metric='cosine',
        spec={'serverless': {'cloud': 'aws', 'region': 'us-east-1'}}  # Add serverless configuration
    )
index = pinecone.Index(PINECONE_INDEX_NAME)

# Load data
def load_data(data_dir):
    loader = DirectoryLoader(data_dir, glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    return documents

# Split text into chunks
def text_split(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks

# Load and split data
extracted_data = load_data("data/")
text_chunks = text_split(extracted_data)
print("Length of extracted chunks:", len(text_chunks))

# Download embedding model
def download_hugging_face_embedding():
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embedding

embedding = download_hugging_face_embedding()

query_result = embedding.embed_query("Hello world")
print("Length", len(query_result))

# Creating Embeddings for Each of The Text Chunks & storing with metadata
if index.describe_index_stats().get('total_vector_count', 0) == 0:
    print("Upserting documents to the index...")
    vectors = [(str(i), embedding.embed_query(t.page_content), {"text": t.page_content}) for i, t in enumerate(text_chunks)]
    for i in range(0, len(vectors), 100):
        batch = vectors[i:i + 100]
        index.upsert(vectors=batch)
    print("Documents upserted.")
else:
    print("Pinecone index is already populated.")

# Initialize docsearch (Modified)
docsearch = Pinecone.from_existing_index(index_name=PINECONE_INDEX_NAME, embedding=embedding)


prompt_template = """
Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}
Question: {question}

Only return the helpful answer below and nothing else.
Helpful answer:
"""

PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
chain_type_kwargs = {"prompt": PROMPT}

llm = CTransformers(model="Model/llama-2-7b-chat.ggmlv3.q4_0.bin",
                    model_type="llama",
                    config={'max_new_tokens': 512, 'temperature': 0.8})

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)

while True:
    user_input = input(f"Input Prompt: ")
    # result = qa({"query": user_input})  # Deprecated call
    result = qa.invoke({"query": user_input})  # Recommended call
    print("Response: ", result["result"])